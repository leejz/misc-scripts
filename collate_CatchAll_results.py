#!/usr/bin/env python
"""
--------------------------------------------------------------------------------
Created:   Jackson Lee 4/30/12

This script reads a directory of information generated by CatchAll using the 
generate_Catchall_infile.py command and then extracts and summarizes all sample 
information into a excel tab delimited file for graphing.  Note, input data must 
be set up by generate_CatchAll_infile.py
   
usage:
python collate_CatchAll_results.py -i input_directory -o output_file
   
input directory format:
all files are of the form:
/input_directory/rarefaction_depth_replicate/CatchAll.Sample.No/CatchAll.Sample.No_BestModelsAnalysis.csv
   
BestModelsAnalysis.csv file format:
a proprietary csv CatchAll Output text file
Sobs = xxx, Model Name, Tau, Sobs, Est Total Sp, SE, Low Bound, High Bound, GOF0, GOF5,
Best Model, ModelName, 39,18.9,521.0,83.0,394.2,.111,.1251,
Each Model...
etc...
   
outfile format:
a tab delimited file with each sample, sample depth, average total species, and SD total species
output_file.csv
   
Sample Name	Subsample Depth	Avg Total Est Sp	SD Total Est Sp
Eq.1	500	21	5
Eq.1	750	25	10
etc...
   
Ob.1	500	250	125
Ob.2	750	280	221
etc...
   
etc. for each sample

--------------------------------------------------------------------------------
usage:    python collate_CatchAll_results.py -i input_directory -o output_file
"""

#-------------------------------------------------------------------------------
#Functions & Declarations

from string import strip
from argparse import ArgumentParser, RawDescriptionHelpFormatter
import os
import csv
from numpy import average, std

#-------------------------------------------------------------------------------
#Body
print("Running...")

if __name__ == '__main__':
    parser = ArgumentParser(usage = "python collate_CatchAll_results.py -i \
input_directory -o output_file",
                            description=__doc__, 
                            formatter_class=RawDescriptionHelpFormatter)
    parser.add_argument("-i", "--input_directory", action="store", 
                        dest="input_dir",
                        help="input directory of CatchAll files (txt format)")
    parser.add_argument("-o", "--output_file", action="store", dest="output_file",
                        help="output file (tab delimited format)")
    options = parser.parse_args()

    mandatories = ["input_dir", "output_file"]
    for m in mandatories:
        if not options.__dict__[m]:
            print("\nERROR: Missing Arguments\n")
            parser.print_help()
            exit(-1)
    
    # read in command line args and parse path and files
    input_dir = "./"+options.input_dir
    outputfilename = options.output_file
    
    if not os.path.exists(input_dir):
        print("\nERROR: Input path does not exist\n")
        parser.print_help()
        exit(-1)
    else:
        #parse directories into their depth cohort
        dir_depths = {}
        listdir = os.listdir(input_dir)
        listdir.sort()
        for directory in listdir:
            if os.path.isdir(input_dir + "/" + directory) == True:
                depth = directory.strip().split("_")[1]
                if depth in dir_depths:
                    dir_depths[depth].append(directory)
                else:
                    dir_depths[depth] = [directory]
                    
    #for each depth cohort directory read in sub dirs and open the file
    dir_depth_list = dir_depths.items()
    dir_depth_list.sort()
    data_dict = {}
    for depth, depth_cohort in dir_depth_list:
        sample_avg_dict = {}
        sample_total_dict = {}
        for directory in depth_cohort:
            raredir = input_dir + "/" + directory
            if os.path.isdir(raredir) == True:
                CA_dir = os.listdir(raredir)
                for subdirectory in CA_dir:
                    if os.path.isdir(raredir + "/" + subdirectory) == True:
                        #open and grab Est Tot Species
                        samplename = subdirectory.strip().split("CatchAll.")[1]
                        filenamepath = raredir + "/" + subdirectory + "/" + subdirectory + "_BestModelsAnalysis.csv"
                        with open(filenamepath,'U') as readerfile:
                            reader = csv.reader(readerfile, dialect="excel")
                            dummy = reader.next() #first two lines useless
                            dummy = reader.next()
                            bestmodelline = reader.next()
                            if len (bestmodelline) > 4:  # some models have no fit and no results
                                EstTotSp = float(bestmodelline[4])
                            else:
                                EstToSp = 0.0
                            if samplename in sample_total_dict:
                                sample_total_dict[samplename].append(EstTotSp)
                            else:
                                sample_total_dict[samplename] = [EstTotSp]
        #average values and save for later
        for samplename, ETSlist in sample_total_dict.items():
            averageETS = average(ETSlist)
            SDETS = std(ETSlist)
            if samplename in data_dict:
                data_dict[samplename].append([float(depth), averageETS, SDETS])
            else:
                data_dict[samplename] = [[float(depth), averageETS, SDETS]]
                        
                        
    print("Writing output.\n")
    with open(outputfilename,'w') as writerfile:
        writer = csv.writer(writerfile, dialect="excel-tab")
        datalist = data_dict.items()
        datalist.sort()
        firstrow = ["Sample Name", "Subsample Depth", "Avg Total Est Sp", "SD Total Est Sp"]
        writer.writerow(firstrow)
        for samplename, sample_run in datalist:
            sample_run.sort()
            for sample_line in sample_run:
                writer.writerow([samplename] + sample_line)
            writer.writerow([])

    print("File: " + outputfilename + " has been written.")
    
    print("Done!")
